# Online Learning Configuration
# This configuration runs ONLY online learning with a pre-trained model
# To use this config:
# 1. Ensure model_path points to a valid model file (check 'ls -la experiments/results/checkpoints/')
# 2. Run: python3 main.py online_learning -c configs/online_learning_config.yaml -m experiments/results/nonlinear_tracking_sineaccel_base_model/checkpoints/final_SubspaceNet_20250613_184942.pt -o experiments/results/online_learning_test_with_noise

system_model:
  N: 9  # number of antennas
  M: 3   # number of sources
  T: 200  # number of snapshots
  snr: 10  # signal-to-noise ratio in dB
  field_type: "far"  # "near" or "far"
  signal_nature: "non-coherent"  # "non-coherent" or "coherent"
  signal_type: "narrowband"  # "narrowband" only supported currently
  wavelength: 1  # carrier wavelength in meters
  eta: 0.0  # steering vector uniform error variance
  bias: 0  # steering vector bias error
  sv_noise_var: 0.0  # steering vector additive gaussian error noise variance
  doa_range: 120  # range of DOA values [-doa_range, doa_range]
  doa_resolution: 1  # resolution of DOA values in degrees
  max_range_ratio_to_limit: 0.5  # ratio of maximum range to Fraunhofer distance
  range_resolution: 1  # resolution of range values in meters

dataset:
  samples_size: 256  # overall dataset size
  test_validation_train_split: [0.2, 0.2, 0.6]  # proportions for [test, validation, train] datasets
  create_data: true  # whether to create new data or use existing data
  save_dataset: false  # whether to save the dataset
  true_doa_train: null  # predefined angles for training (null for random)
  true_range_train: null  # predefined ranges for training (null for random)
  true_doa_test: null  # predefined angles for testing (null for random)
  true_range_test: null  # predefined ranges for testing (null for random)

model:
  type: "SubspaceNet"  # SubspaceNet, DCD-MUSIC
  params:
    diff_method: "esprit"  # esprit, music_1D, music_2D, beamformer
    train_loss_type: "rmspe"  # music_spectrum, rmspe
    tau: 8  # number of autocorrelation lags
    field_type: "Far"  # Far, Near
    regularization: "null"  # aic, mdl, threshold, null
    variant: "small"  # big, small
    norm_layer: false
    batch_norm: false

# Disable regular training
training:
  enabled: false
  epochs: 30
  batch_size: 256
  optimizer: "Adam"
  scheduler: "ReduceLROnPlateau"
  learning_rate: 0.001
  weight_decay: 1e-9
  step_size: 50
  gamma: 0.5
  training_objective: "angle"
  save_checkpoint: true

# CRITICAL: Model loading must happen before online learning
# Keep load_model true and provide a valid model path
simulation:
  train_model: false
  evaluate_model: false
  load_model: true  # IMPORTANT: Must be true to load model before online learning
  save_model: true
  plot_results: true
  save_plots: true
  # Use an existing model file found in the checkpoints directory
  model_path: "experiments/results/nonlinear_tracking_sineaccel_base_model/checkpoints/final_SubspaceNet_20250613_184942.pt"  # Latest model from june 6, 2025
  subspace_methods: []  # No classic methods needed for online learning
  simulation_name: "online_learning_sineaccel_nonlinear"
evaluation: {}

# Enable trajectory with longer sequence for online learning
trajectory:
  enabled: true
  trajectory_type: "sine_accel_nonlinear"  # Type of trajectory for online learning
  # Sine acceleration non-linear model parameters (now oscillatory with source-specific patterns)
  sine_accel_omega0: [-0.15, 0.25, 0.15]  # Frequency of oscillation (rad/s) - different for each source
  sine_accel_kappa: [3, -3, 2]  # Amplitude of oscillation (rad) - different for each source  
  sine_accel_noise_std: 0.03   # Noise standard deviation (rad) - controls randomness
  save_trajectory: false

kalman_filter:
  process_noise_std_dev: 0.03  # standard deviation of process noise (degrees), null to use trajectory.random_walk_std_dev
  measurement_noise_std_dev: 0.03  # standard deviation of measurement noise (degrees)
  initial_covariance: 0.001  # initial state covariance

# Enable online learning with appropriate parameters
# Note: Online learning now uses window-averaged backpropagation where losses are accumulated
# over the entire window before performing gradient steps, providing more stable training.
online_learning:
  enabled: true  # Enable online learning
  window_size: 5  # Size of sliding window
  stride: 5  # Step size for sliding window
  loss_threshold: 0.2  # Threshold for detecting drift and triggering online learning
  max_iterations: 10  # Maximum iterations for online training
  learning_rate: 0.001  # Learning rate for online training (smaller than main training)
  trajectory_length: 300  # Length of trajectory specifically for online learning
  dataset_size: 1  # Number of trajectories for online learning
  
  # Dynamic Eta Update Parameters
  eta_update_interval_windows: 20  # Update eta every N windows. If null or 0, eta is not periodically updated
  eta_increment: 0.9  # Amount to increment (if positive) or decrement (if negative) eta by when an update occurs
  max_eta: 0.9  # Maximum allowed value for eta during dynamic updates
  min_eta: 0.0  # Minimum allowed value for eta during dynamic updates
  
  # Calibration error control
  use_nominal: true  # If true (default), nominal array configuration (no calibration errors) is used for sample generation
  
  # Loss configuration
  loss_config:
    metric: "rmspe"  # Loss metric to use: 'rmspe' or 'rmape'
    supervision: "supervised"  # Supervision mode: 'supervised' (compare with ground truth) or 'unsupervised' (compare with pre-EKF predictions)
    training_loss_type: "unsupervised_rmape"  # Training loss type: 'configured' (use metric+supervision), 'kalman_innovation' (use K*innovation loss), or 'y_s_inv_y' (use y*S^-1*y loss)
  
  # Example for using Kalman innovation loss:
  # loss_config:
  #   metric: "rmape"  # Still used for evaluation, not training
  #   supervision: "unsupervised"  # Still used for evaluation, not training  
  #   training_loss_type: "kalman_innovation"  # Use K*innovation loss for training
  
  # Example for using Y*S^-1*Y loss:
  # loss_config:
  #   metric: "rmspe"  # Still used for evaluation, not training
  #   supervision: "supervised"  # Still used for evaluation, not training  
  #   training_loss_type: "y_s_inv_y"  # Use y*S^-1*y loss for training
  
  # Example for using unsupervised RMAPE loss:
  # loss_config:
  #   metric: "rmspe"  # Still used for evaluation, not training
  #   supervision: "supervised"  # Still used for evaluation, not training  
  #   training_loss_type: "unsupervised_rmape"  # Use unsupervised RMAPE loss for training
  
  # Example for using unsupervised RMSPE loss:
  # loss_config:
  #   metric: "rmspe"  # Still used for evaluation, not training
  #   supervision: "supervised"  # Still used for evaluation, not training  
  #   training_loss_type: "unsupervised_rmspe"  # Use unsupervised RMSPE loss for training
  
  # Online learning start configuration
  time_to_learn: 35  # Window index at which to start online learning

# Logging configuration
logging:
  level: "INFO"  # Global logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  subspace_net_level: null  # Override for SubspaceNet loggers (null = use global level)
  kalman_filter_level: "WARNING"  # Override for Kalman filter loggers (set to WARNING to reduce noise)
  torch_level: "WARNING"  # PyTorch logging level (WARNING to reduce noise)
  matplotlib_level: "WARNING"  # Matplotlib logging level (WARNING to reduce noise)
  log_to_file: false  # Whether to log to a file in addition to console
  log_file_path: null  # Path for log file (null = use default)
  log_file_level: "DEBUG"  # Logging level for file output

# No scenarios defined here, as this config is for a single online learning run
scenarios: null
train_base_model_only_once: true 
scenario_config: null 